{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "53926fa6",
      "metadata": {
        "id": "53926fa6"
      },
      "source": [
        "<img src='http://www-scf.usc.edu/~ghasemig/images/sharif.png' alt=\"SUT logo\" width=200 height=200 align=left class=\"saturate\" >\n",
        "\n",
        "<br>\n",
        "<font face=\"Times New Roman\">\n",
        "<div dir=ltr align=center>\n",
        "<font color=0F5298 size=7>\n",
        "    Introduction to Machine Learning <br>\n",
        "<font color=2565AE size=5>\n",
        "    Computer Engineering Department <br>\n",
        "    Fall 2022<br>\n",
        "<font color=3C99D size=5>\n",
        "    Homework 5: Practical - Recurrent Neural Networks <br>\n",
        "<font color=696880 size=4>\n",
        "    Alireza Farashah, Parsa Hosseini\n",
        "    \n",
        "    \n",
        "____\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4e46a899",
      "metadata": {
        "id": "4e46a899"
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "### Full Name : Neda Fallah\n",
        "### Student Number : 98100226\n",
        "___"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1301d17c",
      "metadata": {
        "id": "1301d17c"
      },
      "source": [
        "# Image Captioning using Recurrent Neural Networks\n",
        "In this notebook we want to implement a deep neural network to caption Flickr images. It has 8091 images and each image in this dataset has an ID and there are 5 caption for each image in captions.txt file which is uploaded in Quera. We use pretrained ResNet50 model to get meaningful features from each image. Code of this part is implemented and you have to use id_to_feature and id_to_caption dictionary. See the code to understand what are these two dictionaries.\n",
        "You have to implement the empty parts of this notebook."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "u64poS4QqwsM",
      "metadata": {
        "id": "u64poS4QqwsM"
      },
      "source": [
        "# Import Libaries and Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "80ddd818",
      "metadata": {
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "execution": {
          "iopub.execute_input": "2022-09-12T07:15:53.978015Z",
          "iopub.status.busy": "2022-09-12T07:15:53.976537Z",
          "iopub.status.idle": "2022-09-12T07:16:01.577213Z",
          "shell.execute_reply": "2022-09-12T07:16:01.575888Z"
        },
        "id": "80ddd818",
        "papermill": {
          "duration": 7.61511,
          "end_time": "2022-09-12T07:16:01.580821",
          "exception": false,
          "start_time": "2022-09-12T07:15:53.965711",
          "status": "completed"
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.applications.resnet import preprocess_input\n",
        "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "import tensorflow.keras as keras\n",
        "\n",
        "from tensorflow.keras import metrics\n",
        "from tensorflow.keras import utils\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.models import Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "FLMIvIHd8vQH",
      "metadata": {
        "id": "FLMIvIHd8vQH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5db68326-5d5a-4240-9711-31fa0bb9babc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-02-08 20:20:07--  https://github.com/jbrownlee/Datasets/releases/download/Flickr8k/Flickr8k_Dataset.zip\n",
            "Resolving github.com (github.com)... 20.205.243.166\n",
            "Connecting to github.com (github.com)|20.205.243.166|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://objects.githubusercontent.com/github-production-release-asset-2e65be/124585957/47f52b80-3501-11e9-8f49-4515a2a3339b?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20230208%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20230208T202007Z&X-Amz-Expires=300&X-Amz-Signature=f78c2008b2c935958956bb5105786e0683e5371b6138c621a8175a39a99fdd74&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=124585957&response-content-disposition=attachment%3B%20filename%3DFlickr8k_Dataset.zip&response-content-type=application%2Foctet-stream [following]\n",
            "--2023-02-08 20:20:08--  https://objects.githubusercontent.com/github-production-release-asset-2e65be/124585957/47f52b80-3501-11e9-8f49-4515a2a3339b?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20230208%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20230208T202007Z&X-Amz-Expires=300&X-Amz-Signature=f78c2008b2c935958956bb5105786e0683e5371b6138c621a8175a39a99fdd74&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=124585957&response-content-disposition=attachment%3B%20filename%3DFlickr8k_Dataset.zip&response-content-type=application%2Foctet-stream\n",
            "Resolving objects.githubusercontent.com (objects.githubusercontent.com)... 185.199.109.133, 185.199.108.133, 185.199.110.133, ...\n",
            "Connecting to objects.githubusercontent.com (objects.githubusercontent.com)|185.199.109.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1115419746 (1.0G) [application/octet-stream]\n",
            "Saving to: ‘Flickr8k_Dataset.zip’\n",
            "\n",
            "Flickr8k_Dataset.zi  65%[============>       ] 696.86M  --.-KB/s    in 2m 53s  \n",
            "\n",
            "2023-02-08 20:23:01 (4.03 MB/s) - Connection closed at byte 730710016. Retrying.\n",
            "\n",
            "--2023-02-08 20:23:02--  (try: 2)  https://objects.githubusercontent.com/github-production-release-asset-2e65be/124585957/47f52b80-3501-11e9-8f49-4515a2a3339b?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20230208%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20230208T202007Z&X-Amz-Expires=300&X-Amz-Signature=f78c2008b2c935958956bb5105786e0683e5371b6138c621a8175a39a99fdd74&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=124585957&response-content-disposition=attachment%3B%20filename%3DFlickr8k_Dataset.zip&response-content-type=application%2Foctet-stream\n",
            "Connecting to objects.githubusercontent.com (objects.githubusercontent.com)|185.199.109.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 206 Partial Content\n",
            "Length: 1115419746 (1.0G), 384709730 (367M) remaining [application/octet-stream]\n",
            "Saving to: ‘Flickr8k_Dataset.zip’\n",
            "\n",
            "Flickr8k_Dataset.zi  65%[+++++++++++++       ] 696.86M  --.-KB/s    in 60s     \n",
            "\n",
            "2023-02-08 20:24:05 (0.00 B/s) - Connection closed at byte 730710016. Retrying.\n",
            "\n",
            "--2023-02-08 20:24:07--  (try: 3)  https://objects.githubusercontent.com/github-production-release-asset-2e65be/124585957/47f52b80-3501-11e9-8f49-4515a2a3339b?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20230208%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20230208T202007Z&X-Amz-Expires=300&X-Amz-Signature=f78c2008b2c935958956bb5105786e0683e5371b6138c621a8175a39a99fdd74&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=124585957&response-content-disposition=attachment%3B%20filename%3DFlickr8k_Dataset.zip&response-content-type=application%2Foctet-stream\n",
            "Connecting to objects.githubusercontent.com (objects.githubusercontent.com)|185.199.109.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 206 Partial Content\n",
            "Length: 1115419746 (1.0G), 384709730 (367M) remaining [application/octet-stream]\n",
            "Saving to: ‘Flickr8k_Dataset.zip’\n",
            "\n",
            "Flickr8k_Dataset.zi  65%[+++++++++++++       ] 696.86M  --.-KB/s               "
          ]
        }
      ],
      "source": [
        "!wget https://github.com/jbrownlee/Datasets/releases/download/Flickr8k/Flickr8k_Dataset.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "FqXRIVtZ9L4z",
      "metadata": {
        "id": "FqXRIVtZ9L4z"
      },
      "outputs": [],
      "source": [
        "!unzip /content/Flickr8k_Dataset.zip -d /content/images/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "YLZjWvvD9TFr",
      "metadata": {
        "id": "YLZjWvvD9TFr"
      },
      "outputs": [],
      "source": [
        "!unzip /content/captions.txt.zip -d /content/text/"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f4fa7248",
      "metadata": {
        "id": "f4fa7248",
        "papermill": {
          "duration": 0.008777,
          "end_time": "2022-09-12T07:16:01.628030",
          "exception": false,
          "start_time": "2022-09-12T07:16:01.619253",
          "status": "completed"
        },
        "tags": []
      },
      "source": [
        "# Creeating Features and Captions Dcitionary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d7939fdf",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-09-12T07:16:01.650450Z",
          "iopub.status.busy": "2022-09-12T07:16:01.649410Z",
          "iopub.status.idle": "2022-09-12T07:16:28.089491Z",
          "shell.execute_reply": "2022-09-12T07:16:28.087838Z"
        },
        "id": "d7939fdf",
        "papermill": {
          "duration": 26.455949,
          "end_time": "2022-09-12T07:16:28.094528",
          "exception": false,
          "start_time": "2022-09-12T07:16:01.638579",
          "status": "completed"
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "resnet50 = tf.keras.applications.ResNet50(\n",
        "    include_top=False,\n",
        "    weights=\"imagenet\",\n",
        "    pooling='avg'\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "35435bd7",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-09-12T07:16:28.138720Z",
          "iopub.status.busy": "2022-09-12T07:16:28.138309Z",
          "iopub.status.idle": "2022-09-12T07:26:46.715435Z",
          "shell.execute_reply": "2022-09-12T07:26:46.713936Z"
        },
        "id": "35435bd7",
        "papermill": {
          "duration": 618.601811,
          "end_time": "2022-09-12T07:26:46.718344",
          "exception": false,
          "start_time": "2022-09-12T07:16:28.116533",
          "status": "completed"
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "#Keeps the features of images\n",
        "id_to_features = {}\n",
        "\n",
        "for name in tqdm(os.listdir(\"/content/images/Flicker8k_Dataset\")):\n",
        "    img = img_to_array(load_img(\"/content/images/Flicker8k_Dataset/\" + name, target_size=(224,224)))\n",
        "    img = preprocess_input(img.reshape((1, img.shape[0], img.shape[1], img.shape[2])))\n",
        "    feature = resnet50.predict(img, verbose=0)\n",
        "    id_to_features[name.split('.')[0]]=feature"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ODYdtjEyd7x1",
      "metadata": {
        "id": "ODYdtjEyd7x1"
      },
      "outputs": [],
      "source": [
        "id_to_features['587604325_af5d6df679'].shape"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "wf6UGbcmvRIW"
      },
      "id": "wf6UGbcmvRIW"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0hbj6NEBjkHE",
      "metadata": {
        "id": "0hbj6NEBjkHE"
      },
      "outputs": [],
      "source": [
        "len(id_to_features)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d2aeb597",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-09-12T07:26:47.778050Z",
          "iopub.status.busy": "2022-09-12T07:26:47.776633Z",
          "iopub.status.idle": "2022-09-12T07:26:47.914147Z",
          "shell.execute_reply": "2022-09-12T07:26:47.912526Z"
        },
        "id": "d2aeb597",
        "papermill": {
          "duration": 0.163806,
          "end_time": "2022-09-12T07:26:47.919278",
          "exception": false,
          "start_time": "2022-09-12T07:26:47.755472",
          "status": "completed"
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "#Keeps the captions of images\n",
        "from itertools import islice\n",
        "\n",
        "id_to_captions = {}\n",
        "\n",
        "with open(\"captions.txt\", 'r') as f:\n",
        "    for i, line in enumerate(f):\n",
        "        if i==0:\n",
        "            continue\n",
        "        tokens = line.split('.jpg,')\n",
        "        id, caption = tokens[0], tokens[1]\n",
        "        if id not in id_to_captions:\n",
        "            id_to_captions[id] = []\n",
        "        id_to_captions[id].append(caption) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6LUn9YSsileO",
      "metadata": {
        "id": "6LUn9YSsileO"
      },
      "outputs": [],
      "source": [
        "len(id_to_captions)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Rv6akvdLjp4Z",
      "metadata": {
        "id": "Rv6akvdLjp4Z"
      },
      "outputs": [],
      "source": [
        "id_to_captions['587604325_af5d6df679']"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dRHfMFqkgrqO",
      "metadata": {
        "id": "dRHfMFqkgrqO"
      },
      "source": [
        "# Preprocessing and Tokenizing (15 points)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "15516079",
      "metadata": {
        "id": "15516079"
      },
      "source": [
        "\n",
        "\n",
        "In this part you need to clean the captions text in order to use Tokenizer.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "989863f0",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-09-12T07:26:48.015073Z",
          "iopub.status.busy": "2022-09-12T07:26:48.013736Z",
          "iopub.status.idle": "2022-09-12T07:26:48.023556Z",
          "shell.execute_reply": "2022-09-12T07:26:48.022040Z"
        },
        "id": "989863f0",
        "papermill": {
          "duration": 0.035062,
          "end_time": "2022-09-12T07:26:48.026890",
          "exception": false,
          "start_time": "2022-09-12T07:26:47.991828",
          "status": "completed"
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "# preprocess caption texts: lowercase/remove special character/remove stop words/tokenize\n",
        "from tqdm import tqdm\n",
        "import re\n",
        "def preprocessing():\n",
        "    for image_id , captions in tqdm(id_to_captions.items()):\n",
        "        for i in range(len(captions)):\n",
        "          #lowercase\n",
        "            captions[i] = re.sub('[\"\\n]', '', captions[i]).lower()\n",
        "          \n",
        "            captions[i] = re.sub('\\s+', ' ', captions[i]).strip()\n",
        "          #delete unnecessary\n",
        "            captions[i] = re.sub('[^A-Za-z\\s]', '', captions[i])\n",
        "          #add start and end\n",
        "            captions[i] = 'startseq ' + \" \".join([word for word in captions[i].split() if len(word) > 1]) + ' endseq'\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "preprocessing()"
      ],
      "metadata": {
        "id": "R_nACJ5f-vIs"
      },
      "id": "R_nACJ5f-vIs",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "ca8f906a",
      "metadata": {
        "id": "ca8f906a",
        "papermill": {
          "duration": 0.02011,
          "end_time": "2022-09-12T07:26:49.435567",
          "exception": false,
          "start_time": "2022-09-12T07:26:49.415457",
          "status": "completed"
        },
        "tags": []
      },
      "source": [
        "## Data Generator (25 points)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "27a084ed",
      "metadata": {
        "id": "27a084ed"
      },
      "source": [
        "In this part implement a data generator function to use during the training.\n",
        "For each image and caption you have to create number of train samples in the following format.\n",
        "\n",
        "caption: \"start_char the man is playing football end_char\"\n",
        "\n",
        "sample 0: X=\"start_char\", Y=\"the \"\n",
        "\n",
        "sample 1: X=\"start_char the\", Y=\"man\"\n",
        "\n",
        "...\n",
        "\n",
        "sample n: X=\"start_char the man is playing football\", Y=\"end_char\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "all_captions = []\n",
        "for image_id, captions in tqdm(id_to_captions.items()):\n",
        "  for caption in captions:\n",
        "    all_captions.append(caption)\n",
        "\n",
        "\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(all_captions)\n",
        "vocabsize = len(tokenizer .word_index) + 1#\n",
        "\n",
        "#max_len = max(len(caption.split()) for caption in all_captions)\n",
        "\n",
        "max_length = len(max(all_captions, key=len))"
      ],
      "metadata": {
        "id": "3-YO_Yn7_TeZ"
      },
      "id": "3-YO_Yn7_TeZ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f899750d",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-09-12T07:26:49.582585Z",
          "iopub.status.busy": "2022-09-12T07:26:49.582264Z",
          "iopub.status.idle": "2022-09-12T07:26:49.594406Z",
          "shell.execute_reply": "2022-09-12T07:26:49.592996Z"
        },
        "id": "f899750d",
        "papermill": {
          "duration": 0.036461,
          "end_time": "2022-09-12T07:26:49.597494",
          "exception": false,
          "start_time": "2022-09-12T07:26:49.561033",
          "status": "completed"
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "# craete data generator \n",
        "\n",
        "def data_generator(batch_size):\n",
        "\n",
        "    data_rows = []\n",
        "    midx = 0\n",
        "\n",
        "    while True:\n",
        "\n",
        "        for i, key in enumerate(id_to_captions):\n",
        "            midx += 1\n",
        "\n",
        "            captions = id_to_captions[key]\n",
        "            for caption in captions:\n",
        "\n",
        "                sequence = tokenizer.texts_to_sequences([caption])[0]\n",
        "                for i in range(1, len(sequence)):\n",
        "\n",
        "                    in_seq, out_seq = sequence[:i], sequence[i]\n",
        "                    in_seq = pad_sequences([in_seq], maxlen=max_length)[0]\n",
        "                    out_seq = utils.to_categorical([out_seq], num_classes=vocabsize)[0]\n",
        "                    data_rows.append((id_to_features[key][0], in_seq, out_seq))\n",
        "\n",
        "            if midx >= batch_size:\n",
        "\n",
        "                features, sequence, target = zip(*data_rows)\n",
        "                data_rows = []\n",
        "                midx = 0\n",
        "\n",
        "                features = np.array(features)\n",
        "                sequence = np.array(sequence)\n",
        "                target = np.array(target)\n",
        "\n",
        "                yield [features, sequence], target\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6dfd3d4d",
      "metadata": {
        "id": "6dfd3d4d",
        "papermill": {
          "duration": 0.01991,
          "end_time": "2022-09-12T07:26:49.637375",
          "exception": false,
          "start_time": "2022-09-12T07:26:49.617465",
          "status": "completed"
        },
        "tags": []
      },
      "source": [
        "## Model (20 points)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ab34c2ca",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-09-12T07:26:49.681861Z",
          "iopub.status.busy": "2022-09-12T07:26:49.680448Z",
          "iopub.status.idle": "2022-09-12T07:26:52.284945Z",
          "shell.execute_reply": "2022-09-12T07:26:52.283035Z"
        },
        "id": "ab34c2ca",
        "papermill": {
          "duration": 2.631023,
          "end_time": "2022-09-12T07:26:52.289020",
          "exception": false,
          "start_time": "2022-09-12T07:26:49.657997",
          "status": "completed"
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "# define the model\n",
        "input_features = layers.Input(shape=(2048,))\n",
        "x2 = layers.Dense(256, activation='relu')(input_features)\n",
        "x3 = layers.Dropout(0.2)(x2)\n",
        "\n",
        "input_sequence = layers.Input(shape=(max_length,))\n",
        "y1 = layers.Embedding(vocabsize, 256, mask_zero=True)(input_sequence)\n",
        "y4 = layers.LSTM(256)(y1)\n",
        "\n",
        "z1 = layers.add([x3, y4])\n",
        "z3 = layers.Dense(256, activation='relu')(z1)\n",
        "\n",
        "outputs = layers.Dense(vocabsize, activation='softmax')(z3)\n",
        "\n",
        "model = Model(inputs=[input_features, input_sequence], outputs=outputs)\n",
        "model.compile(loss='categorical_crossentropy', \n",
        "              optimizer='adam', \n",
        "              metrics=[metrics.CategoricalAccuracy()],)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "NyEgK3y9rOlD",
      "metadata": {
        "id": "NyEgK3y9rOlD"
      },
      "source": [
        "# Train (10 points)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "QaAEqZyssKiO",
      "metadata": {
        "id": "QaAEqZyssKiO"
      },
      "source": [
        "Train the model and plot loss for training data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d60d2bb4",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-09-12T07:26:52.334652Z",
          "iopub.status.busy": "2022-09-12T07:26:52.334286Z",
          "iopub.status.idle": "2022-09-12T08:07:11.417439Z",
          "shell.execute_reply": "2022-09-12T08:07:11.416072Z"
        },
        "id": "d60d2bb4",
        "papermill": {
          "duration": 2419.10956,
          "end_time": "2022-09-12T08:07:11.420929",
          "exception": false,
          "start_time": "2022-09-12T07:26:52.311369",
          "status": "completed"
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "# train the model\n",
        "counter = 0\n",
        "\n",
        "for i, key in enumerate(id_to_captions):\n",
        "    captions = id_to_captions[key]\n",
        "    for caption in captions:\n",
        "        sequence = tokenizer.texts_to_sequences([caption])[0]\n",
        "        for i in range(1, len(sequence)):\n",
        "            counter += 1\n",
        "\n",
        "counter\n",
        "\n",
        "\n",
        "generator = data_generator(32)\n",
        "history = model.fit(generator, epochs=5, steps_per_epoch=len(id_to_captions) // 16, verbose=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "02a65027",
      "metadata": {
        "id": "02a65027",
        "papermill": {
          "duration": 0.46612,
          "end_time": "2022-09-12T08:07:13.291934",
          "exception": false,
          "start_time": "2022-09-12T08:07:12.825814",
          "status": "completed"
        },
        "tags": []
      },
      "source": [
        "## Test (30)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "HjKjXlleraKw",
      "metadata": {
        "id": "HjKjXlleraKw"
      },
      "source": [
        "In this part evaluate the model and generate caption for 10 of test images. For \n",
        "evaluation use \"bert-base-uncased\" pretrained model and calculate similarity for the predicted sentence of model and real captions. \n",
        "Show 3 of the images and predictied samples."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install transformers"
      ],
      "metadata": {
        "id": "ezVKkGrkEc0z"
      },
      "id": "ezVKkGrkEc0z",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "45d48e38",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-09-12T08:07:15.157355Z",
          "iopub.status.busy": "2022-09-12T08:07:15.156843Z",
          "iopub.status.idle": "2022-09-12T08:15:23.399242Z",
          "shell.execute_reply": "2022-09-12T08:15:23.397465Z"
        },
        "id": "45d48e38",
        "papermill": {
          "duration": 488.544828,
          "end_time": "2022-09-12T08:15:23.402162",
          "exception": false,
          "start_time": "2022-09-12T08:07:14.857334",
          "status": "completed"
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "\n",
        "bert_tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\",)\n",
        "bert_model = AutoModel.from_pretrained(\"bert-base-uncased\",output_hidden_states=True)\n",
        "\n",
        "\n",
        "def get_embeddings(text,token_length):\n",
        "    tensor = bert_tokenizer([text], return_tensors='pt', truncation=True, padding=True, max_length=token_length)\n",
        "    output = bert_model(**tensor)\n",
        "    return output[1].cpu().detach().numpy()\n",
        "\n",
        "def calculate_similarity(text1,text2,token_length=20):\n",
        "\n",
        "  embeddings1 = get_embeddings(text1,token_length)\n",
        "  embeddings2 = get_embeddings(text2,token_length)\n",
        "  return cosine_similarity(embeddings1.reshape(1,-1), embeddings2.reshape(1,-1))[0][0]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_caption(model, image_features, max_len):\n",
        "    #image_features = id_to_features[image_id]\n",
        "    in_seq = 'startseq'\n",
        "    for i in range(max_len):\n",
        "\n",
        "        sequence = tokenizer.texts_to_sequences([in_seq])\n",
        "        sequence = pad_sequences(sequence, maxlen=max_length)\n",
        "        sequence = np.array(sequence)\n",
        "\n",
        "        y_pred = model.predict([image_features, sequence], verbose=0)[0]\n",
        "        y_pred = np.argmax(y_pred)\n",
        "        \n",
        "        token = tokenizer.sequences_to_texts([[y_pred]])[0]\n",
        "        in_seq += f' {token}'\n",
        "\n",
        "        if token == 'endseq':\n",
        "            break\n",
        "\n",
        "    return \" \".join(in_seq.split()[1:-1])"
      ],
      "metadata": {
        "id": "iMRljBnYBa9g"
      },
      "id": "iMRljBnYBa9g",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\"\"\"\"\n",
        "ids = list(id_to_features)\n",
        "\n",
        "for id in ids[:10]:\n",
        "\n",
        "    actual = [' '.join(c.split(' ')[1:-1]) for c in id_to_captions[id]]\n",
        "    print('actual:', actual)\n",
        "\n",
        "    prediction = predict_caption(model, id, max_length)\n",
        "    print('predicted:', prediction)\n",
        "\n",
        "    similarity = sum(calculate_similarity(caption, prediction, 50) for caption in actual) / len(actual)\n",
        "    print('similarity:', similarity, '\\n')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 539
        },
        "id": "Ei32gGm_BNRO",
        "outputId": "5779c901-c473-47b9-d87b-8fa0d350c47c"
      },
      "id": "Ei32gGm_BNRO",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "actual: ['warmly dressed person is shoveling snow outside house', 'young girl is shoveling snow in front of house', 'child shovels snow near bush outside of building', 'the child is shoveling deep white snow near the house', 'the girl wearing brown jacket whilst walking in snow']\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-35-a5399fb580c3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'actual:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactual\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mprediction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredict_caption\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_length\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'predicted:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprediction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-30-43f04c233d7b>\u001b[0m in \u001b[0;36mpredict_caption\u001b[0;34m(model, image_id, max_len)\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0msequence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msequence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m         \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mimage_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msequence\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m         \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1267\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodules\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1268\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mmodules\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1269\u001b[0;31m         raise AttributeError(\"'{}' object has no attribute '{}'\".format(\n\u001b[0m\u001b[1;32m   1270\u001b[0m             type(self).__name__, name))\n\u001b[1;32m   1271\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'BertModel' object has no attribute 'predict'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "image_id_list = list(id_to_features.keys())\n",
        "fig, ax = plt.subplots(nrows=2, ncols=1)\n",
        "ax = ax.flatten()\n",
        "random_list = tuple(np.random.randint(low=1, high=len(image_id_list), size=10))\n",
        "for i in range(2):\n",
        "    k = image_id_list[random_list[i]]\n",
        "    image_feature = id_to_features[k]\n",
        "    actual_labels = [' '.join(c.split(' ')[1:-1]) for c in id_to_captions[k]]\n",
        "    s1 = 'actual labels:'+ actual_labels[0] +\"\\n\"\n",
        "    print(\"\\n\")\n",
        "\n",
        "    prediction_label = predict_caption(model, image_feature, max_length)\n",
        "    s2 = 'predicted label:'+ prediction_label +\"\\n\"\n",
        "    print(\"\\n\")\n",
        "    \n",
        "    similarity = 0\n",
        "    for caption in actual_labels:\n",
        "        similarity += calculate_similarity(caption, prediction_label, 50) \n",
        "    \n",
        "    similarity /= len(actual_labels) \n",
        "    s3 = 'avg similarity score:'+ str(similarity)+\"\\n\"\n",
        "    \n",
        "\n",
        "    img = plt.imread(\"/content/images/Flicker8k_Dataset/\" + k+ \".jpg\")\n",
        "    ax[i].imshow(img)\n",
        "    ax[i].set_title(s1+s2+s3)\n",
        "\n",
        "\n",
        "plt.suptitle(\"images\")\n",
        "plt.savefig('samples.png', bbox_inches='tight')\n",
        "plt.show()    "
      ],
      "metadata": {
        "id": "fTROO7gvGUb4"
      },
      "id": "fTROO7gvGUb4",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "u64poS4QqwsM"
      ],
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.12"
    },
    "papermill": {
      "default_parameters": {},
      "duration": 3593.217349,
      "end_time": "2022-09-12T08:15:36.207538",
      "environment_variables": {},
      "exception": null,
      "input_path": "__notebook__.ipynb",
      "output_path": "__notebook__.ipynb",
      "parameters": {},
      "start_time": "2022-09-12T07:15:42.990189",
      "version": "2.3.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}